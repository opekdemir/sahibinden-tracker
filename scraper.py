# -*- coding: utf-8 -*-
"""fethiye villa

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UJsIYoW0kusDGFDQK-psZgTBxre3gjcB
"""

# Install required packages
!pip install gspread google-auth bs4 requests --quiet

import requests
from bs4 import BeautifulSoup
import gspread
from google.colab import auth
import google.auth
from datetime import datetime

# Authenticate and connect to Google Sheets
auth.authenticate_user()
creds, _ = google.auth.default()
gc = gspread.authorize(creds)

# Connect to your sheet
sheet_name = "Sahibinden Villa Tracker"
try:
    sh = gc.open(sheet_name)
except gspread.exceptions.SpreadsheetNotFound:
    sh = gc.create(sheet_name)
    sh.share('', perm_type='anyone', role='writer')  # Optional: make public

ws = sh.worksheet("Listings")

# Add headers if sheet is empty
if not ws.get_all_values():
    ws.append_row(["Description", "Price", "Ad ID", "Ad Date", "Location", "Timestamp"])

# Scrape listings from sahibinden.com
def scrape_sahibinden():
    url = "https://www.sahibinden.com/satilik-villa/mugla-fethiye?sorting=price_asc"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
    }

    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, "html.parser")
    listings = soup.select("tr.searchResultsItem")

    data = []
    for item in listings:
        try:
            ad_id = item.get("data-id")
            desc = item.select_one(".classifiedTitle a").get_text(strip=True)
            price = item.select_one(".searchResultsPriceValue").get_text(strip=True).replace("TL", "").strip()
            date = item.select_one(".searchResultsDateValue").get_text(strip=True)
            location = item.select_one(".searchResultsLocationValue").get_text(strip=True).replace("\n", " / ").strip()

            row = [desc, price, ad_id, date, location, datetime.now().strftime("%Y-%m-%d %H:%M:%S")]
            data.append(row)
        except:
            continue
    return data

# Load existing data for change tracking
existing = ws.get_all_values()[1:]  # Skip header
existing_keys = {(row[2], row[0], row[1]) for row in existing}  # (ad_id, desc, price)

# Scrape and compare
new_rows = scrape_sahibinden()
added_count = 0

for row in new_rows:
    key = (row[2], row[0], row[1])
    if key not in existing_keys:
        ws.append_row(row)
        added_count += 1

print(f"âœ… Done! {added_count} new or changed listings added to Google Sheets.")

